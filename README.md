# Signans

Although communication is the foundation of human society, the ongoing COVID-19 pandemic has imposed a large amount of restrictions worldwide, and in-person contacts are reduced to the minimum essential. However, the growing popularity of video calls systems has provided a convenient palliative solution regarding remote personal and professional communications. 
Although, people who rely on sign language, facial expressions, and lipreading to communicate are experiencing difficulty using such tools during the coronavirus outbreak. In an attempt to maximize inclusion and expand the level of communication between signans and non-signans, this project employs the use of machine learning to translate American Sign Language fingerspelled alphabet into text and speech in real-time. The proposed system is designed to be an extension plug-in for well-established video conferencing services, which will have the option to translate the word or sentence to a written/spoken target language. It identifies and classifies ASL signs, and translates it to speech using Google Translate's text-to-speech API. The product showed to be sturdy and could classify signs with over 85% certainty almost instantaneously, depending on the imageâ€™s background. Performance decrease was an issue when the background had a lot of noise. Considering this issue can be easily remediated by the user, the model was deemed production ready, with the option of mobile and web application deployment through TensorFlow JS and TensorFlow Lite respectively.